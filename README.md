# MLP From Scratch vs Sklearn

Este projeto compara duas abordagens de treinamento de uma Rede Neural Multicamadas (MLP) para classifica√ß√£o de dados:
- **Usando o Scikit-Learn (MLPClassifier)**
- **Criando uma MLP manualmente utilizando apenas NumPy**

O objetivo √© analisar a diferen√ßa de desempenho (precis√£o e loss) entre uma implementa√ß√£o manual e uma biblioteca consolidada.

---

## üìö O que o projeto faz

- Soma d√≠gitos de diversos n√∫meros RMS para escolher dinamicamente um **dataset do OpenML**.
- Baixa e pr√©-processa o dataset.
- Treina duas redes:
  - Uma **MLP** com o **Scikit-Learn**.
  - Uma **MLP manual** feita com **NumPy**.
- Compara a **precis√£o** de ambas.
- Gera **gr√°ficos** de loss e acur√°cia do treinamento da MLP manual.
- Mostra um **gr√°fico de compara√ß√£o final** entre as duas abordagens.

---

## üöÄ Como executar

**Pr√©-requisitos:**
- Python 3.8+
- Pacotes:
  - `numpy`
  - `matplotlib`
  - `scikit-learn`
  - `openml`

Instale as depend√™ncias:
```bash
pip install numpy matplotlib scikit-learn openml
```

**Execute o script principal:**
```bash
python main.py
```

---

## üß† Estrutura do Projeto

- **openml**: utilizado para baixar datasets automaticamente.
- **scikit-learn**:
  - Treinamento do modelo com `MLPClassifier`.
  - Divis√£o treino/teste com `train_test_split`.
  - Pr√©-processamento com `LabelEncoder`.
  - C√°lculo de precis√£o (`accuracy_score`).
- **NumPy**:
  - Implementa√ß√£o manual da rede.
  - Forward pass, c√°lculo de loss (entropia cruzada) e backpropagation.
- **Matplotlib**:
  - Gr√°ficos de evolu√ß√£o do treinamento (Loss e Acur√°cia).
  - Gr√°fico de compara√ß√£o entre os dois modelos.

---

## üìà Resultados Esperados

- **Precis√£o (accuracy)** semelhante entre a MLP manual e a do Scikit-Learn (se tudo estiver funcionando corretamente).
- Gr√°ficos como:
  - Evolu√ß√£o da loss e acur√°cia durante o treinamento manual.
  - Compara√ß√£o final de precis√£o entre as duas abordagens.

---

## ‚öôÔ∏è Poss√≠veis Melhorias Futuras

- Implementar regulariza√ß√µes (L2, dropout) na rede manual.
- Otimizar o treinamento com algoritmos como **Adam** ou **Momentum**.
- Testar diferentes fun√ß√µes de ativa√ß√£o (tanh, leaky ReLU).
- Suportar m√∫ltiplas classes na sa√≠da (atualmente √© bin√°rio).

---

## üìÑ Licen√ßa

Este projeto √© livre para uso pessoal e educacional.

---

## üìà Resultados Gr√°ficos

### 1. Evolu√ß√£o do Loss e Acur√°cia

![Gr√°fico de Loss e Acur√°cia](assets/loss_acc_durante_treinamento.png)

Este gr√°fico mostra como a rede neural manual treinou ao longo do tempo. 
- O loss caiu gradualmente indicando aprendizado.
- A acur√°cia subiu, mostrando que a rede melhorou nas previs√µes.

---

### 2. Compara√ß√£o de Precis√£o entre Modelos

![Gr√°fico de Compara√ß√£o de Precis√£o](assets/Comp_precis√£o_dos_modelos.png)

Aqui comparamos diretamente o desempenho:
- A MLP manual (NumPy) apresentou uma precis√£o muito pr√≥xima √† do Scikit-Learn.
- Isso valida a qualidade da implementa√ß√£o manual.

---

## üß™ Resultados do Terminal

- Precis√£o Sklearn: **0.9875**
- Precis√£o Manual: **0.9875**
- Diferen√ßa: **0.0000** ‚úÖ
